{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\HP\\\\Desktop\\\\Machine Learning Programing with prajwal'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\HP\\\\Downloads\\\\Batch 93 Day25\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\HP\\\\Downloads\\\\Batch 93 Day25'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary  Purchased \n",
      "0   France   44.0  72000.0         No\n",
      "1       NaN  27.0  48000.0        Yes\n",
      "2  Germany    NaN  54000.0         No\n",
      "3    Spain   39.0      NaN         No\n",
      "4  Germany    NaN  64000.0        Yes\n",
      "5   France   35.0  58000.0        Yes\n",
      "6    Spain   39.0  52000.0         No\n",
      "7   France   48.0      NaN        Yes\n",
      "8  Germany   50.0  83000.0         No\n",
      "9   France   37.0  67000.0        Yes\n"
     ]
    }
   ],
   "source": [
    "df1=pd.read_excel('Data Preprocessing Data File.xlsx')\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France ' 44.0 72000.0]\n",
      " [nan 27.0 48000.0]\n",
      " ['Germany ' nan 54000.0]\n",
      " ['Spain ' 39.0 nan]\n",
      " ['Germany ' nan 64000.0]\n",
      " ['France ' 35.0 58000.0]\n",
      " ['Spain ' 39.0 52000.0]\n",
      " ['France ' 48.0 nan]\n",
      " ['Germany ' 50.0 83000.0]\n",
      " ['France ' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "#Load independent and dependent variables to two sepearate arrays\n",
    "'''x is independent variable'''\n",
    "x=df1.iloc[:,0:-1].values\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''y is dependent variable'''\n",
    "y=df1.iloc[:,3].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France ' 44.0 72000.0]\n",
      " ['France ' 27.0 48000.0]\n",
      " ['Germany ' nan 54000.0]\n",
      " ['Spain ' 39.0 nan]\n",
      " ['Germany ' nan 64000.0]\n",
      " ['France ' 35.0 58000.0]\n",
      " ['Spain ' 39.0 52000.0]\n",
      " ['France ' 48.0 nan]\n",
      " ['Germany ' 50.0 83000.0]\n",
      " ['France ' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n",
    "imputer=imputer.fit(x[:,0:1])\n",
    "x[:,0:1]=imputer.transform(x[:,0:1])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France ' 44.0 72000.0]\n",
      " ['France ' 27.0 48000.0]\n",
      " ['Germany ' 40 54000.0]\n",
      " ['Spain ' 39.0 nan]\n",
      " ['Germany ' 40 64000.0]\n",
      " ['France ' 35.0 58000.0]\n",
      " ['Spain ' 39.0 52000.0]\n",
      " ['France ' 48.0 nan]\n",
      " ['Germany ' 50.0 83000.0]\n",
      " ['France ' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Age - Constant\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer(missing_values=np.nan,strategy='constant',fill_value=40)\n",
    "imputer=imputer.fit(x[:,1:2])\n",
    "x[:,1:2]=imputer.transform(x[:,1:2])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France ' 44.0 72000.0]\n",
      " ['France ' 27.0 48000.0]\n",
      " ['Germany ' 40.0 54000.0]\n",
      " ['Spain ' 39.0 nan]\n",
      " ['Germany ' 40.0 64000.0]\n",
      " ['France ' 35.0 58000.0]\n",
      " ['Spain ' 39.0 52000.0]\n",
      " ['France ' 48.0 nan]\n",
      " ['Germany ' 50.0 83000.0]\n",
      " ['France ' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Age - Populate with mean -When there is no outlier.........Ok Prajwal \n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer()\n",
    "imputer=imputer.fit(x[:,1:2])\n",
    "x[:,1:2]=imputer.transform(x[:,1:2])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France ' 44.0 72000.0]\n",
      " ['France ' 27.0 48000.0]\n",
      " ['Germany ' 40.0 54000.0]\n",
      " ['Spain ' 39.0 62250.0]\n",
      " ['Germany ' 40.0 64000.0]\n",
      " ['France ' 35.0 58000.0]\n",
      " ['Spain ' 39.0 52000.0]\n",
      " ['France ' 48.0 62250.0]\n",
      " ['Germany ' 50.0 83000.0]\n",
      " ['France ' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# salary - Mean\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "imputer=imputer.fit(x[:,2:3])\n",
    "x[:,2:3]=imputer.transform(x[:,2:3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France ' 44.0 72000.0]\n",
      " ['France ' 27.0 48000.0]\n",
      " ['Germany ' 40.0 54000.0]\n",
      " ['Spain ' 39.0 62250.0]\n",
      " ['Germany ' 40.0 64000.0]\n",
      " ['France ' 35.0 58000.0]\n",
      " ['Spain ' 39.0 52000.0]\n",
      " ['France ' 48.0 62250.0]\n",
      " ['Germany ' 50.0 83000.0]\n",
      " ['France ' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "imputer=imputer.fit(x[:,2:3])\n",
    "x[:,2:3]=imputer.transform(x[:,2:3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['France ', 'France ', 'Germany ', 'Spain ', 'Germany ', 'France ',\n",
       "       'Spain ', 'France ', 'Germany ', 'France '], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France ', 44.0, 72000.0],\n",
       "       ['France ', 27.0, 48000.0],\n",
       "       ['Germany ', 40.0, 54000.0],\n",
       "       ['Spain ', 39.0, 62250.0],\n",
       "       ['Germany ', 40.0, 64000.0],\n",
       "       ['France ', 35.0, 58000.0],\n",
       "       ['Spain ', 39.0, 52000.0],\n",
       "       ['France ', 48.0, 62250.0],\n",
       "       ['Germany ', 50.0, 83000.0],\n",
       "       ['France ', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 44.0 72000.0]\n",
      " [0 27.0 48000.0]\n",
      " [1 40.0 54000.0]\n",
      " [2 39.0 62250.0]\n",
      " [1 40.0 64000.0]\n",
      " [0 35.0 58000.0]\n",
      " [2 39.0 52000.0]\n",
      " [0 48.0 62250.0]\n",
      " [1 50.0 83000.0]\n",
      " [0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# I want label encoding in country columns......\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "x[:,0]=le.fit_transform(x[:,0])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "ley=LabelEncoder()\n",
    "y=ley.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 44.0, 72000.0],\n",
       "       [0, 27.0, 48000.0],\n",
       "       [1, 40.0, 54000.0],\n",
       "       [2, 39.0, 62250.0],\n",
       "       [1, 40.0, 64000.0],\n",
       "       [0, 35.0, 58000.0],\n",
       "       [2, 39.0, 52000.0],\n",
       "       [0, 48.0, 62250.0],\n",
       "       [1, 50.0, 83000.0],\n",
       "       [0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "transformers : list of tuples\n",
    " |      List of (name, transformer, columns) tuples specifying the\n",
    " |      transformer objects to be applied to subsets of the data.'''\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct=ColumnTransformer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ColumnTransformer in module sklearn.compose._column_transformer:\n",
      "\n",
      "class ColumnTransformer(sklearn.base.TransformerMixin, sklearn.utils.metaestimators._BaseComposition)\n",
      " |  ColumnTransformer(transformers, *, remainder='drop', sparse_threshold=0.3, n_jobs=None, transformer_weights=None, verbose=False, verbose_feature_names_out=True)\n",
      " |  \n",
      " |  Applies transformers to columns of an array or pandas DataFrame.\n",
      " |  \n",
      " |  This estimator allows different columns or column subsets of the input\n",
      " |  to be transformed separately and the features generated by each transformer\n",
      " |  will be concatenated to form a single feature space.\n",
      " |  This is useful for heterogeneous or columnar data, to combine several\n",
      " |  feature extraction mechanisms or transformations into a single transformer.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <column_transformer>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.20\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  transformers : list of tuples\n",
      " |      List of (name, transformer, columns) tuples specifying the\n",
      " |      transformer objects to be applied to subsets of the data.\n",
      " |  \n",
      " |      name : str\n",
      " |          Like in Pipeline and FeatureUnion, this allows the transformer and\n",
      " |          its parameters to be set using ``set_params`` and searched in grid\n",
      " |          search.\n",
      " |      transformer : {'drop', 'passthrough'} or estimator\n",
      " |          Estimator must support :term:`fit` and :term:`transform`.\n",
      " |          Special-cased strings 'drop' and 'passthrough' are accepted as\n",
      " |          well, to indicate to drop the columns or to pass them through\n",
      " |          untransformed, respectively.\n",
      " |      columns :  str, array-like of str, int, array-like of int,                 array-like of bool, slice or callable\n",
      " |          Indexes the data on its second axis. Integers are interpreted as\n",
      " |          positional columns, while strings can reference DataFrame columns\n",
      " |          by name.  A scalar string or int should be used where\n",
      " |          ``transformer`` expects X to be a 1d array-like (vector),\n",
      " |          otherwise a 2d array will be passed to the transformer.\n",
      " |          A callable is passed the input data `X` and can return any of the\n",
      " |          above. To select multiple columns by name or dtype, you can use\n",
      " |          :obj:`make_column_selector`.\n",
      " |  \n",
      " |  remainder : {'drop', 'passthrough'} or estimator, default='drop'\n",
      " |      By default, only the specified columns in `transformers` are\n",
      " |      transformed and combined in the output, and the non-specified\n",
      " |      columns are dropped. (default of ``'drop'``).\n",
      " |      By specifying ``remainder='passthrough'``, all remaining columns that\n",
      " |      were not specified in `transformers`, but present in the data passed\n",
      " |      to `fit` will be automatically passed through. This subset of columns\n",
      " |      is concatenated with the output of the transformers. For dataframes,\n",
      " |      extra columns not seen during `fit` will be excluded from the output\n",
      " |      of `transform`.\n",
      " |      By setting ``remainder`` to be an estimator, the remaining\n",
      " |      non-specified columns will use the ``remainder`` estimator. The\n",
      " |      estimator must support :term:`fit` and :term:`transform`.\n",
      " |      Note that using this feature requires that the DataFrame columns\n",
      " |      input at :term:`fit` and :term:`transform` have identical order.\n",
      " |  \n",
      " |  sparse_threshold : float, default=0.3\n",
      " |      If the output of the different transformers contains sparse matrices,\n",
      " |      these will be stacked as a sparse matrix if the overall density is\n",
      " |      lower than this value. Use ``sparse_threshold=0`` to always return\n",
      " |      dense.  When the transformed output consists of all dense data, the\n",
      " |      stacked result will be dense, and this keyword will be ignored.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of jobs to run in parallel.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  transformer_weights : dict, default=None\n",
      " |      Multiplicative weights for features per transformer. The output of the\n",
      " |      transformer is multiplied by these weights. Keys are transformer names,\n",
      " |      values the weights.\n",
      " |  \n",
      " |  verbose : bool, default=False\n",
      " |      If True, the time elapsed while fitting each transformer will be\n",
      " |      printed as it is completed.\n",
      " |  \n",
      " |  verbose_feature_names_out : bool, default=True\n",
      " |      If True, :meth:`get_feature_names_out` will prefix all feature names\n",
      " |      with the name of the transformer that generated that feature.\n",
      " |      If False, :meth:`get_feature_names_out` will not prefix any feature\n",
      " |      names and will error if feature names are not unique.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  transformers_ : list\n",
      " |      The collection of fitted transformers as tuples of\n",
      " |      (name, fitted_transformer, column). `fitted_transformer` can be an\n",
      " |      estimator, 'drop', or 'passthrough'. In case there were no columns\n",
      " |      selected, this will be the unfitted transformer.\n",
      " |      If there are remaining columns, the final element is a tuple of the\n",
      " |      form:\n",
      " |      ('remainder', transformer, remaining_columns) corresponding to the\n",
      " |      ``remainder`` parameter. If there are remaining columns, then\n",
      " |      ``len(transformers_)==len(transformers)+1``, otherwise\n",
      " |      ``len(transformers_)==len(transformers)``.\n",
      " |  \n",
      " |  named_transformers_ : :class:`~sklearn.utils.Bunch`\n",
      " |      Read-only attribute to access any transformer by given name.\n",
      " |      Keys are transformer names and values are the fitted transformer\n",
      " |      objects.\n",
      " |  \n",
      " |  sparse_output_ : bool\n",
      " |      Boolean flag indicating whether the output of ``transform`` is a\n",
      " |      sparse matrix or a dense numpy array, which depends on the output\n",
      " |      of the individual transformers and the `sparse_threshold` keyword.\n",
      " |  \n",
      " |  output_indices_ : dict\n",
      " |      A dictionary from each transformer name to a slice, where the slice\n",
      " |      corresponds to indices in the transformed output. This is useful to\n",
      " |      inspect which transformer is responsible for which transformed\n",
      " |      feature(s).\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`. Only defined if the\n",
      " |      underlying transformers expose such an attribute when fit.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  make_column_transformer : Convenience function for\n",
      " |      combining the outputs of multiple transformer objects applied to\n",
      " |      column subsets of the original feature space.\n",
      " |  make_column_selector : Convenience function for selecting\n",
      " |      columns based on datatype or the columns name with a regex pattern.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The order of the columns in the transformed feature matrix follows the\n",
      " |  order of how the columns are specified in the `transformers` list.\n",
      " |  Columns of the original feature matrix that are not specified are\n",
      " |  dropped from the resulting transformed feature matrix, unless specified\n",
      " |  in the `passthrough` keyword. Those columns specified with `passthrough`\n",
      " |  are added at the right to the output of the transformers.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.compose import ColumnTransformer\n",
      " |  >>> from sklearn.preprocessing import Normalizer\n",
      " |  >>> ct = ColumnTransformer(\n",
      " |  ...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n",
      " |  ...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n",
      " |  >>> X = np.array([[0., 1., 2., 2.],\n",
      " |  ...               [1., 1., 0., 1.]])\n",
      " |  >>> # Normalizer scales each row of X to unit norm. A separate scaling\n",
      " |  >>> # is applied for the two first and two last elements of each\n",
      " |  >>> # row independently.\n",
      " |  >>> ct.fit_transform(X)\n",
      " |  array([[0. , 1. , 0.5, 0.5],\n",
      " |         [0.5, 0.5, 0. , 1. ]])\n",
      " |  \n",
      " |  :class:`ColumnTransformer` can be configured with a transformer that requires\n",
      " |  a 1d array by setting the column to a string:\n",
      " |  \n",
      " |  >>> from sklearn.feature_extraction import FeatureHasher\n",
      " |  >>> from sklearn.preprocessing import MinMaxScaler\n",
      " |  >>> import pandas as pd   # doctest: +SKIP\n",
      " |  >>> X = pd.DataFrame({\n",
      " |  ...     \"documents\": [\"First item\", \"second one here\", \"Is this the last?\"],\n",
      " |  ...     \"width\": [3, 4, 5],\n",
      " |  ... })  # doctest: +SKIP\n",
      " |  >>> # \"documents\" is a string which configures ColumnTransformer to\n",
      " |  >>> # pass the documents column as a 1d array to the FeatureHasher\n",
      " |  >>> ct = ColumnTransformer(\n",
      " |  ...     [(\"text_preprocess\", FeatureHasher(input_type=\"string\"), \"documents\"),\n",
      " |  ...      (\"num_preprocess\", MinMaxScaler(), [\"width\"])])\n",
      " |  >>> X_trans = ct.fit_transform(X)  # doctest: +SKIP\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ColumnTransformer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.utils._set_output._SetOutputMixin\n",
      " |      sklearn.utils.metaestimators._BaseComposition\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, transformers, *, remainder='drop', sparse_threshold=0.3, n_jobs=None, transformer_weights=None, verbose=False, verbose_feature_names_out=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit all transformers using X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, dataframe} of shape (n_samples, n_features)\n",
      " |          Input data, of which specified subsets are used to fit the\n",
      " |          transformers.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,...), default=None\n",
      " |          Targets for supervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : ColumnTransformer\n",
      " |          This estimator.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit all transformers, transform the data and concatenate results.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, dataframe} of shape (n_samples, n_features)\n",
      " |          Input data, of which specified subsets are used to fit the\n",
      " |          transformers.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,), default=None\n",
      " |          Targets for supervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_t : {array-like, sparse matrix} of                 shape (n_samples, sum_n_components)\n",
      " |          Horizontally stacked results of transformers. sum_n_components is the\n",
      " |          sum of n_components (output dimension) over transformers. If\n",
      " |          any result is a sparse matrix, everything will be converted to\n",
      " |          sparse matrices.\n",
      " |  \n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Input features.\n",
      " |      \n",
      " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      " |            used as feature names in. If `feature_names_in_` is not defined,\n",
      " |            then the following input feature names are generated:\n",
      " |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      " |          - If `input_features` is an array-like, then `input_features` must\n",
      " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Transformed feature names.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Returns the parameters given in the constructor as well as the\n",
      " |      estimators contained within the `transformers` of the\n",
      " |      `ColumnTransformer`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_output(self, *, transform=None)\n",
      " |      Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n",
      " |      \n",
      " |      Calling `set_output` will set the output of all estimators in `transformers`\n",
      " |      and `transformers_`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      transform : {\"default\", \"pandas\"}, default=None\n",
      " |          Configure output of `transform` and `fit_transform`.\n",
      " |      \n",
      " |          - `\"default\"`: Default output format of a transformer\n",
      " |          - `\"pandas\"`: DataFrame output\n",
      " |          - `None`: Transform configuration is unchanged\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  set_params(self, **kwargs)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      Valid parameter keys can be listed with ``get_params()``. Note that you\n",
      " |      can directly set the parameters of the estimators contained in\n",
      " |      `transformers` of `ColumnTransformer`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : ColumnTransformer\n",
      " |          This estimator.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X separately by each transformer, concatenate results.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, dataframe} of shape (n_samples, n_features)\n",
      " |          The data to be transformed by subset.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_t : {array-like, sparse matrix} of                 shape (n_samples, sum_n_components)\n",
      " |          Horizontally stacked results of transformers. sum_n_components is the\n",
      " |          sum of n_components (output dimension) over transformers. If\n",
      " |          any result is a sparse matrix, everything will be converted to\n",
      " |          sparse matrices.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  named_transformers_\n",
      " |      Access the fitted transformer by name.\n",
      " |      \n",
      " |      Read-only attribute to access any transformer by given name.\n",
      " |      Keys are transformer names and values are the fitted transformer\n",
      " |      objects.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |  \n",
      " |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from abc.ABCMeta\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "help(ColumnTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' transformers : list of tuples\n",
    " |      List of (name, transformer, columns) tuples specifying the\n",
    " |      transformer objects to be applied to subsets of the data.\n",
    " |  '''\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct=ColumnTransformer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct=ColumnTransformer(transformers=[('onehot',OneHotEncoder(),[0])])\n",
    "ct.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [1.0, 0.0, 0.0, 27.0, 48000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 54000.0],\n",
       "       [0.0, 0.0, 1.0, 39.0, 62250.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 64000.0],\n",
       "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
       "       [0.0, 0.0, 1.0, 39.0, 52000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 62250.0],\n",
       "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct=ColumnTransformer(transformers=[('onehot',OneHotEncoder(),[0])],remainder='passthrough')\n",
    "ct.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2     3        4\n",
      "0  1.0  0.0  0.0  44.0  72000.0\n",
      "1  1.0  0.0  0.0  27.0  48000.0\n",
      "2  0.0  1.0  0.0  40.0  54000.0\n",
      "3  0.0  0.0  1.0  39.0  62250.0\n",
      "4  0.0  1.0  0.0  40.0  64000.0\n",
      "5  1.0  0.0  0.0  35.0  58000.0\n",
      "6  0.0  0.0  1.0  39.0  52000.0\n",
      "7  1.0  0.0  0.0  48.0  62250.0\n",
      "8  0.0  1.0  0.0  50.0  83000.0\n",
      "9  1.0  0.0  0.0  37.0  67000.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct=ColumnTransformer(transformers=[('onehot',OneHotEncoder(),[0])],remainder='passthrough')\n",
    "x=ct.fit_transform(x)\n",
    "print(pd.DataFrame(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [1.0, 0.0, 0.0, 27.0, 48000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 54000.0],\n",
       "       [0.0, 0.0, 1.0, 39.0, 62250.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 64000.0],\n",
       "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
       "       [0.0, 0.0, 1.0, 39.0, 52000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 62250.0],\n",
       "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0         1    2         3         4\n",
      "0  1.0 -0.654654 -0.5  0.660861  1.002707\n",
      "1  1.0 -0.654654 -0.5 -2.079293 -1.465494\n",
      "2 -1.0  1.527525 -0.5  0.016119 -0.848444\n",
      "3 -1.0 -0.654654  2.0 -0.145067  0.000000\n",
      "4 -1.0  1.527525 -0.5  0.016119  0.179973\n",
      "5  1.0 -0.654654 -0.5 -0.789809 -0.437077\n",
      "6 -1.0 -0.654654  2.0 -0.145067 -1.054127\n",
      "7  1.0 -0.654654 -0.5  1.305603  0.000000\n",
      "8 -1.0  1.527525 -0.5  1.627974  2.133965\n",
      "9  1.0 -0.654654 -0.5 -0.467438  0.488498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_sca=StandardScaler()\n",
    "x_STD=std_sca.fit_transform(x)\n",
    "print(pd.DataFrame(x_STD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3    4\n",
      "0  0.000014  0.000000  0.000000  0.000611  1.0\n",
      "1  0.000021  0.000000  0.000000  0.000562  1.0\n",
      "2  0.000000  0.000019  0.000000  0.000741  1.0\n",
      "3  0.000000  0.000000  0.000016  0.000627  1.0\n",
      "4  0.000000  0.000016  0.000000  0.000625  1.0\n",
      "5  0.000017  0.000000  0.000000  0.000603  1.0\n",
      "6  0.000000  0.000000  0.000019  0.000750  1.0\n",
      "7  0.000016  0.000000  0.000000  0.000771  1.0\n",
      "8  0.000000  0.000012  0.000000  0.000602  1.0\n",
      "9  0.000015  0.000000  0.000000  0.000552  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "Nm_x=Normalizer()\n",
    "x_NOR=Nm_x.fit_transform(x)\n",
    "print(pd.DataFrame(x_NOR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
